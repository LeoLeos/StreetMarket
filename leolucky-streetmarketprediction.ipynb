{"cells":[{"metadata":{},"cell_type":"markdown","source":"（这比赛我才打了没几天，中文选手可以加我微信chixujohnny，可以拉群探讨一下）\n\n# 思路\n\n## 解决一个什么问题？\n预测test数据集中，每一个可交易的时间节点，是做【action】操作还是【pass】操作，可以转化为一个是否要做action的一个二分类问题。<br/>\n我们预测的是当前这个时间节点，在给定了一坨feature之后，当前这个节点是否要action=1的一个二分类。<br/>\n注意不是给定当前feature预测下一个或未来某个时间节点的二分类问题，所以数据加工上会简单一些。<br/>\n\nevaluation原文如下：Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it.\n<br/><br/>\n\n## target是什么\n我觉得探查数据之前应该先明确target。<br/>\n你可能要问了，不就是上面是否要做action的二分类问题吗？<br/>\n但实际上原数据集中给的并不是action这样一个label，而根据题意，每一个时间节点是否发生action并能不能产生reward，是由6个数据共同决定的：\n* weight\n* resp\n* resp_1\n* resp_2\n* resp_3\n* resp_4 \n\n公式为：pi=∑j(weightij∗respij∗actionij)<br/>\n我发现了一个细节，就是evaluation公式里面，只给了一个respij（也就是在某日期下某时间节点的一个收益）<br/>\n这个收益没有明确的表示是哪个resp，因为数据集中给了5种resp<br/>\n所以一般这种“赛方故意说的比较含糊”的点，会成为一个头部队伍拉开差距的地方。<br/><br/>\n既然我们希望这个pi越大越好，且后面做的还是∑，所以我们要尽可能的保证respij是正数。<br/>\n并且简单探查了一下数据发现，weight还有0的，所以【要把这些混淆视听的weight=0的sample全都剃掉】<br/><br/>\n说了这么多那target到底是啥？<br/>\n我觉得是一个比较开放性的判断，可以依次将每种resp都试一下，when resp_x > 0 then 1 else 0<br/>\n可以将output做成5个target，最后对预测的output做个emsemble试试看也行。\n\n## feature怎么加工\n### 缺失值填充\n看了下，130个特征里只有50个特征是完全没有NaN值的。<br/>\n我把所有特征的分布拉了一下，是符合正态分布的，所以用mean填一下就行了。<br/>\n（我还有想法用剩余的50个特征拟合一个Regression预测一下剩余feature里的NaN值，notebook GPU时间受限，暂时没时间搞了，感觉会有点用，不过应该不会有啥飞跃）<br/>\n### 归一化\nXGB的化就不用归一化，这个notebook用的NN，同样看了一下feature分布，都还比较均匀，所以不归一化也行（不排除是赛方提前给我们做了一轮加工？）<br/><br/>\n\n## 用什么model\n这个notebook我用的NN，其实我非常喜欢XGB的，有几个原因让我用了NN：<br/>\n1、公司电脑还回去了，我自己的笔记本的XGB-GPU环境有问题，用optuna寻找最佳超参数太慢了，没时间折腾了。<br/>\n2、notebook GPU时间上限也不算太充裕，不如单机debug来的方便。<br/><br/>\n其实这个问题我觉得如果想从10% -> 1%冲击一下model是可以的，时间短的话别把太多精力放在model上，收益不大，折腾半天还容易overfitting<br/><br/>\n\n## 如果给我一个月时间把这个比赛搞好？\n* 调试5种resp做target比较花时间，这个应该没啥trick只能暴力试一下，毕竟赛方没给我们这几个值在金融领域的含义是啥。<br/>\n* 用Regression把缺失值补一下。<br/>\n* 多模型融合：有个想法是把GRU-embedding、NN-embedding concat起来丢给XGB。<br/>\n* 做一些进阶的数据预处理，比如做一下分箱平滑一下噪音点等等（其实看分布感觉噪音并不明显，强行分箱不排除有效果变差的可能）<br/>\n* 【一些特别玄学的东西很重要】泡在Discussion里多看看别人的思路，比如我看到一个Master说，date=85之前交易频率明显比86~500天要高，反正看大家聊了一通也不知道因为啥，大概率可能是因为交易模型在那个时候发生了改版？原来因为收益不行要较高的频率操作，之后行情好了放慢了频率？等等一些奇怪的猜测...反正我听取的建议，踢掉了前85天的train数据，效果是有提升的，约等于剃掉噪音点，只不过这个噪音是连续的一大坨。<br/>我还看到有人把预测的threshole上调了几个万分点，score又怼上去一点的例子...反正挺玄学的。\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## TODO LIST（1.26更新）\n* 一个离线评估函数，做一下valid调参【doing】\n* 开始做特征工程，先弄点滑窗特征看一下吧，从SQL转pandas真的非常不适应\n* 再开一个notebook好好做一下EDA\n* 有些feature不遵循正态分布，准备单独把这些feature抽出来单独train一下\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n## 一些想法\n* 看了另一个大神的NN模型的讨论区，有一个GM说他fit的模型实际上在A榜是overfitting的，他俩差点没打起来，其实我觉得打初赛刷rank就是要不断的overfitting A榜的feature啊，我之前也有A榜冠军开心的一比，B榜overfitting直接变SB的经历。反正我觉得吧，现在这个Leaderboard没啥太大的参考意义，我觉得头部队伍到了B榜基本全部都要完蛋的节奏（当然如果你只是刷A榜型选手那当我没说）\n* 【玄学事件】有关随机种子的问题我看讨论区也有很多人讨论，不同的随机种子会造成甚至上千的score差异，如果一个模型fit的相对比较完美是不可能出现这种情况的，非常迷，这块我暂时再观察一下讨论区看看，截止到1.26还没什么想法。\n* 这个问题既然是多目标优化问题，那为什么不把weight当成一个target优化呢，我看完全没有人这么做，试一试？"},{"metadata":{},"cell_type":"markdown","source":"## Loading dataset..."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# coding: utf-8\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport datatable as dt\n\n#  初始化一个随机数种子\nSEED = 1111\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nprint('Loading data...')\ntrain_datatable = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv')  # 用datatable会快一点\ntrain = train_datatable.to_pandas()\ndel train_datatable\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data preprocessing...')\n# train = train.query('date > 85').reset_index(drop = True)   # 只保留第86天及以后的data\ntrain = train.query('date > 85').query('ts_id > 13000').reset_index(drop = True)   # 只保留第86天及以后的data\ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\n# train['action'] = ((train['resp'].values) > 0).astype(int)\n\nfeatures = []\nfor item in train.columns:\n    if 'feature' in item:\n        features.append(item)\n\nf_mean = np.mean(train[features[1:]].values,axis=0)  # 算一下每个feature的均值，用于缺失值填充\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make X and y..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Make X and y...')\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\ndel train\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define a easy NN model..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Create a easy NN model...')\n\nHIDDEN_LAYER = [150, 150, 150]\nTARGET_NUM = 5   # 优化那5个resp\n\ninput = tf.keras.layers.Input(shape=(len(features), ))\n\nx = tf.keras.layers.BatchNormalization()(input)\nx = tf.keras.layers.Dropout(0.2)(x)\nfor units in HIDDEN_LAYER:\n    x = tf.keras.layers.Dense(units)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)  # 除了ReLU还可以试试别的，后面可以做一个多模型融合\n    x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(TARGET_NUM)(x)\n\noutput = tf.keras.layers.Activation(\"sigmoid\")(x)\n\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\nmodel.compile(\n    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3),\n    metrics   = tf.keras.metrics.AUC(name=\"AUC\"),\n    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n)\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train NN..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train NN...')\n\nhistory = model.fit(X_train, y_train, epochs=5, batch_size=3000)\nmodels = []\nmodels.append(model)\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## See what is the Learning Curve looks like...\n\nAccroding to this learning curve.<br/>\nMaybe 18 epoch is a better chioce? <br/>\nBut it seems to be in a state of continuous overfitting.<br/>\nChange prams should be a way to increace score.<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('See what is the Learning Curve looks like...')\nimport matplotlib.pyplot as plt\n\n# 绘制训练 & 验证的准确率值\nplt.plot(history.history['AUC'])\n# plt.plot(history.history['val_AUC'])\nplt.title('Model accuracy')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# 绘制训练 & 验证的损失值\nplt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Free the Memory..."},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 自己添加代码1.28\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"testData = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\ntestData = testData.iloc[:,1:-2]\n\n# 通过模型预测出来的label\npredict = model.predict(testData)\n\n# 将数组predict 转成 dataFrame\nDFpredict = pd.DataFrame(predict)\n# print(DFpredict)\n\n# 概览predict\n# print(predict)\n\n# 查看训练集的列名\n# print(X_train.columns.values)\n\n# reshape测试集的feature\n# print(testData.iloc[:,1:-2].columns.values)\n\n# 打印预测集的label shape\n# print(np.shape(predict))\n\n# 打印训练集的label shape\n# print(np.shape(y_train))\n\n# print(testData)\n# print(np.size(predict))\n\n# 合并预测结果和example_sample_submission\nsubmit = pd.read_csv('../input/jane-street-market-prediction/example_sample_submission.csv')\n# print(submit)\nmergeResult = pd.concat([submit, DFpredict],axis=1)\n# print(mergeResult)\n# print(submit)\n\n# 填充Nan\nmergeResult.fillna(mergeResult.mean(),inplace=True)\nprint(mergeResult)\n# 保存预测的结果\nmergeResult.to_csv('./submission.csv')\n\nprint('done')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# THRESHOLD = 0.5   # 试了一个对照组，还是0.5比较好用\n\n# import janestreet\n# from tqdm import tqdm\n# janestreet.make_env.__called__ = False\n# env = janestreet.make_env()\n# for (test_df, pred_df) in tqdm(env.iter_test()):\n#     if test_df['weight'].item() > 0:\n#         test_samples = test_df.loc[:, features].values  # 遍历每个样本的feature部分，做成一个一行130列的matrix\n#         if np.isnan(test_samples[:, 1:].sum()):         # 如果这行样本里面有NaN的话（忽略离散特征feature_0）用train数据集里的mean替换NaN\n#             test_samples[:, 1:] = np.nan_to_num(test_samples[:, 1:]) + np.isnan(test_samples[:, 1:]) * f_mean  \n#         pred = model(test_samples, training = False).numpy()\n        \n#         # version = 1\n#         pred = np.median(pred)\n        \n#         # version = 2\n# #         pred = np.mean(pred)\n        \n#         # version = 3\n# #         pred = pred[3]\n        \n#         pred_df.action = np.where(pred >= THRESHOLD, 1, 0).astype(int)\n#     else:\n#         pred_df.action = 0\n#     env.predict(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}